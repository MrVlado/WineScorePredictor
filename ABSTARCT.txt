
TEAM:   Vlad Iosif & Marcel Transier
PROJECT DATASET:Wine Quality Data Set(https://archive.ics.uci.edu/ml/datasets/Wine+Quality)
PROJECT NAME: WineScorePredictor
GIT REPOSITORY: https://github.com/MrVlado/WineScorePredictor.git





Input variables (based on physicochemical tests):

1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol



OUTPUT
A real number ranged in (0:10) that indicataes the quality





SCHEDULED STEPS

-PREPARE THE DATA & UNDERSTAND IT
    -Print the data(maybe use PCA reduction), and show of the outliers
    -Looking for correlations with the features(Correlation Matrix)
    -Studing the features of the samples in the dataset with linear regression to find positive or negative  correlations with the score(the results has to be adequatly susitained by a small enought loss to be considered valuable)
        -Intresting result in this phase can make us think about feature reduction
        
        
    -Split in training and testing 70/30 (look for a balanced split based on the gaussian distributon of the Initail dataset)
    -Use part of the training as a validation set(k-fold cross validation)
    -Normalize the dataset(every feature between 0:1)

-PREPARE THE MODEL

-Choose the alghoitms 
    -SVM (with different kernels)
        -POLY
        -RADIAL
        -SIGMOIDAL
    - NN (we can try different activation functions, and different layers setup)
        
    - Perceptron Alghorithm
    
-TRAIN THE MODELS
Grid search k-fold cross validation for parameter selection will be applied for every alghoritm tested
    -Tune the hyperparameters of the model before the training using a gridsearch cross validation method.
    -Once we found the best hyperparameters of the models we train on the entire training set and than we validate the result on the testing     set.

-COMPARE THE RESULT OF THE DIFFERENT MODELS
    -Compare the ROC curves and the AUC of the different methods
    -See where one did good and other did bad
    -Consider the option of marging the results of the different models in one singe decision rule.
    